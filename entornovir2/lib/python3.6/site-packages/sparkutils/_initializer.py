import os
import sys
import time
import logging
import glob

from pyspark.sql import SparkSession
from pyspark.context import SparkContext


class PySparkInitializer(object):
    """
    PySparkInitializer class wraps Initializer delegate class included
    in the JVM library, allowing to launch Spark applications
    from the Python side
    """

    def init_spark(self, app_name: str = None) -> SparkSession:
        """
        Initialize the spark session
        :param app_name: The application name. If None, a name
        will be generated automatically
        """
        # create the session
        if not app_name:
            app_name = PySparkInitializer.generate_app_name()

        # Add sparkutils jar to extra classpath
        lib_jars = glob.glob(sys.prefix + os.sep + "share/sparkutils/jars/utils*.jar")
        sparkutils_jar = os.path.join(sys.prefix, ":".join(lib_jars))

        # Start the spark session
        self.__spark = SparkSession\
            .builder\
            .appName(app_name)\
            .config("spark.driver.extraClassPath", sparkutils_jar)\
            .getOrCreate()

        # Enable bidirectional calls between Java and Python processes
        if not self.__spark._sc._gateway.get_callback_server():
            self.__spark._sc._gateway.start_callback_server()
            # INFO: Tracing py4j calls can be enabled by uncommenting the line below
            # self.__spark._sc._gateway.jvm.py4j.GatewayServer.turnLoggingOn()

        return self.__spark

    def is_connected(self):
        """
        Check spark context gateway connection
        """
        return not self.__spark._sc._gateway.get_callback_server().is_shutdown

    def shutdown(self):
        """
        Shutdown spark session and allocated resources
        """
        self.__spark._sc._gateway.shutdown_callback_server()
        self.__spark.stop()
        return not self.is_connected()

    @classmethod
    def generate_app_name(cls) -> str:
        """
        Get the application name based on environment variables
        """
        project = os.getenv("JOB_PROJECT", "projectEmpty")
        application = os.getenv("JOB_APPLICATION", "applicationEmpty")
        component = os.getenv("JOB_COMPONENT", PySparkInitializer._timestamp())
        return "%s-%s-%s" % (project, application, component)

    @classmethod
    def _timestamp(cls, ts=None):
        """
        Get a stringified representation of the current time that can be used for naming the application
        :return the stringifed timestamp
        """
        if not ts:
            ts = time.gmtime()
        return time.strftime("%Y-%m-%d_%H.%M.%S", ts)


class JvmLoggerHandler(logging.Handler):
    """
    Class JvmLogger handler allows logging to log4j loggers
    through a Python logging handler
    """

    # Translation of log levels to log4j calls
    JVM_CALLS = {"DEBUG": "debug",
                 "INFO": "info",
                 "WARNING": "warning",
                 "ERROR": "error",
                 "CRITICAL": "fatal"}

    def emit(self, record):
        """
        See logging.handler
        :param record: the LogRecord objects create by python logging module
        """

        if not SparkContext._jvm:
            return

        logger = SparkContext._jvm.org.apache.log4j.LogManager.getLogger(record.name)
        try:
            getattr(logger, JvmLoggerHandler.JVM_CALLS[record.levelname])(record.msg)
        except Exception:
            sys.stderr.write("Error logging the message from %s: %s" % (record.name, record.msg))
