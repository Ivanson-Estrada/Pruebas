from ._java_runtime import JvmArguments
from .task import PySparkTask
from pyspark.context import SparkContext
from pyspark.sql import SparkSession

import py4j
import logging


class _TaskFacade:
    """
    TaskFacade class abstracts Python processes implementation from inner JVM objects
    """

    TASK_ERROR_CODE = -1

    def __init__(self, task: PySparkTask):
        """
        Constructor
        :param task: the Python object that will perform the task
        """
        self.__task = task

    def runProcess(self, spark, config) -> int:
        """
        Execute the task
        :param spark: the applicable Spark session
        :param config: the process configuration
        :return: an exit code that describes the process results
        """
        # Wrap the SparkSession scala object into a SparkSession provide by pyspark
        app_name = spark.conf().get("spark.app.name")
        pyspark_session = SparkSession.builder.appName(app_name).getOrCreate()
        try:
            return self.__task.runProcess(pyspark_session, config)
        except Exception:
            import traceback
            exc_str = traceback.format_exc()
            print(exc_str)
            return _TaskFacade.TASK_ERROR_CODE

    def defineBusinessInfo(self, config):
        """
        Define the BusinessInformation for the implemented process
        :param config: the process configuration
        :return: the BusinessInformation object for this process
        """
        bi = self.__task.defineBusinessInfo(config)
        return SparkContext._jvm.com.datio.spark.metric.model.BusinessInformation(bi.exitCode,
                                                                                  bi.entity,
                                                                                  bi.path,
                                                                                  bi.mode,
                                                                                  bi.schema,
                                                                                  bi.schemaVersion,
                                                                                  bi.reprocessing)

    def ret_code(self) -> int:
        """
        Get the return code for this task
        This is a convenience method, as spark utils terminates the JVM
        with a System.exit call without notifying the JavaGateway clients
        """
        return self.__task.ret_code()

    class Java:
        """
        Interface declaration
        """
        implements = ["com.datio.spark.IInitSparkDelegate"]


class SparkLauncher:
    """
    Launcher class encapsulates the instantiation of spark-utils classes
    and the process start
    """

    @classmethod
    def _jvm_launcher(cls, spark, task):
        """
        Instantiate the JVM launcher
        :param spark: the spark context
        :param task: the process containing the spark processing
        """
        _task = _TaskFacade(task)
        return spark._jvm.com.datio.spark.InitSparkDelegateLauncher(spark._jsparkSession, _task)

    @classmethod
    def run(cls, spark, task, args) -> int:
        """
        Launch de process
        :param spark: the spark context
        :param task: the process containing the spark processing
        :param args: the JvmArguments instance containing the arguments
        """
        jvm_args = JvmArguments(args)
        jvm_launcher = SparkLauncher._jvm_launcher(spark, task)
        try:
            ret_code = jvm_launcher.runTask(jvm_args.args())
        except py4j.protocol.Py4JError:
            logging.getLogger(__name__).debug("JVM is closed, not expecting response from JVM")
        else:
            return ret_code
