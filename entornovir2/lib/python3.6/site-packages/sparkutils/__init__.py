from ._initializer import PySparkInitializer, JvmLoggerHandler
from ._java_sparkutils import SparkLauncher
from .task import PySparkTask

from pyspark.sql import SparkSession

import logging
from typing import List
import sys

__version__ = '0.2.4'

__all__ = ["run", "PySparkTask"]


class SparkUtilsException(Exception):
    """
    SparkUtils exception
    """
    pass


def _configure_logging():
    """
    Configure logging capabilities.
    Log traces will be dumped to stdout handler
    :return:
    """
    sparkutils_root = logging.getLogger("sparkutils")
    sparkutils_root.handlers = []
    sparkutils_root.setLevel(logging.DEBUG)

    ch = JvmLoggerHandler()
    ch.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    sparkutils_root.addHandler(ch)


def _launch_spark(spark: SparkSession, task: PySparkTask, args: List[str]) -> int:
    """
    Start the InitSpark main process
    """
    # Launch init spark
    return SparkLauncher.run(spark, task, args)


def get_logger(name):
    """
    Get a logger whose name is the given one
    :param name: the logger name
    :return: the logger object associated to the given name
    """
    full_name = "sparkutils." + name
    return logging.getLogger(full_name)


def run(task: PySparkTask, spark_provider: PySparkInitializer = PySparkInitializer()) -> int:
    """
    Perform the task
    :param task: the spark process to execute
    :param spark_provider: the spark context provider
    :return: the result of the process
    """
    if not task:
        get_logger(__name__).fatal("Undefined process")
        raise SparkUtilsException("Undefined process")

    spark = spark_provider.init_spark()
    try:
        ret_code = _launch_spark(spark, task, sys.argv[1:])
    finally:
        spark_provider.shutdown()

    return ret_code


# Configure logging as soon as the package is imported
_configure_logging()
